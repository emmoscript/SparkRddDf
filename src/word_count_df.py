"""
Word Count usando DataFrames en Apache Spark

Este script implementa el algoritmo de Word Count usando el enfoque de DataFrames,
que es la API estructurada de Spark con optimizaci√≥n autom√°tica Catalyst.

Caracter√≠sticas de DataFrames:
- API estructurada similar a SQL
- Optimizaci√≥n autom√°tica con Catalyst
- Mejor rendimiento que RDDs
- Esquema tipado
- Operaciones m√°s expresivas
"""

import os
import sys
import time
from typing import List, Tuple
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, explode, split, count, desc, lower, regexp_replace, trim, length
from utils import create_spark_session, save_results_dataframe, measure_performance


def word_count_dataframe(spark: SparkSession, input_file: str) -> DataFrame:
    """
    Implementa Word Count usando DataFrames
    
    Pipeline DataFrame:
    1. read.text() - Carga el archivo como DataFrame de l√≠neas
    2. select() + split() + explode() - Divide l√≠neas en palabras
    3. groupBy() + count() - Agrupa y cuenta palabras
    4. orderBy() - Ordena por frecuencia
    
    Ventajas sobre RDDs:
    - Optimizaci√≥n autom√°tica con Catalyst
    - API m√°s declarativa
    - Mejor rendimiento
    - Operaciones SQL nativas
    
    Args:
        spark: SparkSession configurada
        input_file: Ruta del archivo de entrada
        
    Returns:
        DataFrame con columnas (word, count) ordenado por frecuencia
    """
    print(f"Procesando archivo: {input_file}")
    
    # 1. CARGAR DATOS
    # read.text() crea un DataFrame donde cada fila es una l√≠nea del archivo
    # La columna se llama "value" por defecto
    lines_df = spark.read.text(input_file)
    print(f"DataFrame de l√≠neas creado: {lines_df.count()} l√≠neas")
    
    # 2. PROCESAR PALABRAS
    # Usar funciones de Spark SQL para procesar el texto
    words_df = lines_df.select(
        # split() divide cada l√≠nea en un array de palabras
        # explode() convierte cada elemento del array en una fila separada
        explode(split(col("value"), " ")).alias("word")
    )
    print(f"DataFrame de palabras creado: {words_df.count()} palabras totales")
    
    # 3. LIMPIAR Y FILTRAR PALABRAS
    clean_words_df = words_df.select(
        # Convertir a min√∫sculas
        lower(col("word")).alias("word")
    ).select(
        # Remover caracteres especiales usando regex
        regexp_replace(col("word"), r'[^\w\s]', '').alias("word")
    ).select(
        # Remover espacios en blanco
        trim(col("word")).alias("word")
    ).filter(
        # Filtrar palabras vac√≠as
        (col("word") != "") & (length(col("word")) > 0)
    )
    
    # 4. CONTAR PALABRAS
    # groupBy() agrupa por palabra
    # count() cuenta las ocurrencias de cada palabra
    word_counts_df = clean_words_df.groupBy("word").count()
    print(f"Word Count completado: {word_counts_df.count()} palabras √∫nicas encontradas")
    
    # 5. ORDENAR RESULTADOS
    # orderBy() ordena por frecuencia descendente
    sorted_word_counts_df = word_counts_df.orderBy(desc("count"))
    
    return sorted_word_counts_df


def word_count_dataframe_sql(spark: SparkSession, input_file: str) -> DataFrame:
    """
    Implementa Word Count usando DataFrames con SQL
    
    Esta versi√≥n muestra c√≥mo usar SQL directamente con DataFrames,
    demostrando la flexibilidad de la API estructurada.
    
    Args:
        spark: SparkSession configurada
        input_file: Ruta del archivo de entrada
        
    Returns:
        DataFrame con resultados del Word Count
    """
    print(f"Procesando archivo con SQL: {input_file}")
    
    # Cargar datos
    lines_df = spark.read.text(input_file)
    
    # Crear vista temporal para usar SQL
    lines_df.createOrReplaceTempView("lines")
    
    # Ejecutar Word Count usando SQL
    sql_query = """
    WITH words AS (
        SELECT explode(split(value, ' ')) as word
        FROM lines
    ),
    clean_words AS (
        SELECT trim(regexp_replace(lower(word), '[^\\w\\s]', '')) as word
        FROM words
        WHERE trim(regexp_replace(lower(word), '[^\\w\\s]', '')) != ''
        AND length(trim(regexp_replace(lower(word), '[^\\w\\s]', ''))) > 0
    )
    SELECT word, count(*) as count
    FROM clean_words
    GROUP BY word
    ORDER BY count DESC
    """
    
    result_df = spark.sql(sql_query)
    print(f"Word Count SQL completado: {result_df.count()} palabras √∫nicas encontradas")
    
    return result_df


def analyze_dataframe_operations(spark: SparkSession, input_file: str):
    """
    Analiza las operaciones DataFrame paso a paso
    
    Args:
        spark: SparkSession configurada
        input_file: Ruta del archivo de entrada
    """
    print("\n" + "="*50)
    print("AN√ÅLISIS DETALLADO DE OPERACIONES DATAFRAME")
    print("="*50)
    
    # Cargar datos
    lines_df = spark.read.text(input_file)
    print(f"1. L√≠neas cargadas: {lines_df.count()}")
    
    # Dividir en palabras
    words_df = lines_df.select(explode(split(col("value"), " ")).alias("word"))
    print(f"2. Palabras extra√≠das: {words_df.count()}")
    
    # Limpiar palabras
    clean_words_df = words_df.select(
        trim(regexp_replace(lower(col("word")), r'[^\w\s]', '')).alias("word")
    ).filter(
        (col("word") != "") & (length(col("word")) > 0)
    )
    print(f"3. Palabras limpias: {clean_words_df.count()}")
    
    # Contar palabras
    word_counts_df = clean_words_df.groupBy("word").count()
    print(f"4. Palabras √∫nicas: {word_counts_df.count()}")
    
    # Mostrar top 10 palabras
    top_words = word_counts_df.orderBy(desc("count")).limit(10).collect()
    print("\nTop 10 palabras m√°s frecuentes:")
    for i, row in enumerate(top_words, 1):
        print(f"{i:2d}. '{row['word']}': {row['count']}")


def compare_dataframe_approaches(spark: SparkSession, input_file: str):
    """
    Compara los dos enfoques de DataFrame (API vs SQL)
    
    Args:
        spark: SparkSession configurada
        input_file: Ruta del archivo de entrada
    """
    print("\n" + "="*60)
    print("COMPARACI√ìN: DATAFRAME API vs SQL")
    print("="*60)
    
    # Enfoque 1: DataFrame API
    print("üîß Usando DataFrame API...")
    df_api, time_api = measure_performance(word_count_dataframe, spark, input_file)
    
    # Enfoque 2: SQL
    print("üìù Usando SQL...")
    df_sql, time_sql = measure_performance(word_count_dataframe_sql, spark, input_file)
    
    print(f"\nResultados de comparaci√≥n:")
    print(f"- DataFrame API: {time_api:.2f} segundos")
    print(f"- SQL: {time_sql:.2f} segundos")
    print(f"- Diferencia: {abs(time_api - time_sql):.2f} segundos")
    
    if time_api < time_sql:
        print("‚úÖ DataFrame API es m√°s r√°pido")
    else:
        print("‚úÖ SQL es m√°s r√°pido")


def main():
    """
    Funci√≥n principal para ejecutar Word Count con DataFrames
    """
    print("=" * 60)
    print("WORD COUNT USANDO DATAFRAMES")
    print("=" * 60)
    
    # Configurar Spark
    spark = create_spark_session("WordCountDataFrame")
    
    # Verificar archivo de entrada
    input_file = "data/large_text.txt"
    if not os.path.exists(input_file):
        print(f"‚ùå Archivo no encontrado: {input_file}")
        print("Ejecuta primero: python src/generate_data.py")
        return
    
    try:
        # Ejecutar Word Count con DataFrame API
        print("\nüöÄ Iniciando Word Count con DataFrames (API)...")
        word_counts_df, execution_time = measure_performance(word_count_dataframe, spark, input_file)
        
        # Guardar resultados
        save_results_dataframe(word_counts_df, "results/df_output.csv")
        
        # Mostrar estad√≠sticas
        print(f"\nüìä Estad√≠sticas:")
        print(f"- Tiempo de ejecuci√≥n: {execution_time:.2f} segundos")
        print(f"- Palabras √∫nicas: {word_counts_df.count()}")
        
        # Mostrar top 10 palabras
        top_words = word_counts_df.limit(10).collect()
        print(f"\nüèÜ Top 10 palabras m√°s frecuentes:")
        for i, row in enumerate(top_words, 1):
            print(f"{i:2d}. '{row['word']}': {row['count']}")
        
        # An√°lisis detallado
        print("\nüîç Ejecutando an√°lisis detallado...")
        analyze_dataframe_operations(spark, input_file)
        
        # Comparar enfoques
        print("\nüîÑ Comparando enfoques DataFrame...")
        compare_dataframe_approaches(spark, input_file)
        
        print(f"\n‚úÖ Word Count con DataFrames completado exitosamente!")
        
    except Exception as e:
        print(f"‚ùå Error durante la ejecuci√≥n: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Cerrar Spark
        spark.stop()
        print("SparkSession cerrada.")


if __name__ == "__main__":
    main() 